{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hoanganh2411/dulieuvahocsau/blob/master/lab7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download_shell()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1p1yDnoeYjN9",
        "outputId": "996116b8-3210-423b-937d-55a5d6901562"
      },
      "id": "1p1yDnoeYjN9",
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> l\n",
            "\n",
            "Packages:\n",
            "  [ ] abc................. Australian Broadcasting Commission 2006\n",
            "  [ ] alpino.............. Alpino Dutch Treebank\n",
            "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
            "  [ ] averaged_perceptron_tagger_eng Averaged Perceptron Tagger (JSON)\n",
            "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
            "  [ ] averaged_perceptron_tagger_rus Averaged Perceptron Tagger (Russian)\n",
            "  [ ] basque_grammars..... Grammars for Basque\n",
            "  [ ] bcp47............... BCP-47 Language Tags\n",
            "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
            "                           Extraction Systems in Biology)\n",
            "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
            "  [ ] book_grammars....... Grammars from NLTK Book\n",
            "  [ ] brown............... Brown Corpus\n",
            "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
            "  [ ] cess_cat............ CESS-CAT Treebank\n",
            "  [ ] cess_esp............ CESS-ESP Treebank\n",
            "  [ ] chat80.............. Chat-80 Data Files\n",
            "  [ ] city_database....... City Database\n",
            "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
            "  [ ] comparative_sentences Comparative Sentence Dataset\n",
            "Hit Enter to continue: averaged_perceptron_tagger Averaged Perceptron Tagger\n",
            "  [ ] comtrans............ ComTrans Corpus Sample\n",
            "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
            "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
            "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
            "                           and Basque Subset)\n",
            "  [ ] crubadan............ Crubadan Corpus\n",
            "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
            "  [ ] dolch............... Dolch Word List\n",
            "  [ ] english_wordnet..... Open English Wordnet\n",
            "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
            "                           Corpus\n",
            "  [ ] extended_omw........ Extended Open Multilingual WordNet\n",
            "  [ ] floresta............ Portuguese Treebank\n",
            "  [ ] framenet_v15........ FrameNet 1.5\n",
            "  [ ] framenet_v17........ FrameNet 1.7\n",
            "  [ ] gazetteers.......... Gazeteer Lists\n",
            "  [ ] genesis............. Genesis Corpus\n",
            "  [ ] gutenberg........... Project Gutenberg Selections\n",
            "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
            "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
            "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
            "Hit Enter to continue: gutenberg........... Project Gutenberg Selections\n",
            "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
            "                           ChaSen format)\n",
            "  [ ] kimmo............... PC-KIMMO Data Files\n",
            "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
            "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
            "                           for parser comparison\n",
            "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
            "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
            "                           part-of-speech tags\n",
            "  [ ] machado............. Machado de Assis -- Obra Completa\n",
            "  [ ] masc_tagged......... MASC Tagged Corpus\n",
            "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
            "  [ ] maxent_ne_chunker_tab ACE Named Entity Chunker (Maximum entropy)\n",
            "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
            "  [ ] maxent_treebank_pos_tagger_tab Treebank Part of Speech Tagger (Maximum entropy)\n",
            "  [ ] moses_sample........ Moses Sample Models\n",
            "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
            "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
            "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
            "                           2015) subset of the Paraphrase Database.\n",
            "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
            "Hit Enter to continue: movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
            "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
            "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
            "  [ ] nps_chat............ NPS Chat\n",
            "  [ ] omw-1.4............. Open Multilingual Wordnet\n",
            "  [ ] omw................. Open Multilingual Wordnet\n",
            "  [ ] opinion_lexicon..... Opinion Lexicon\n",
            "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
            "  [ ] paradigms........... Paradigm Corpus\n",
            "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
            "                           Evaluation Shared Task\n",
            "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
            "                           character properties in Perl\n",
            "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
            "  [ ] pl196x.............. Polish language of the XX century sixties\n",
            "  [ ] porter_test......... Porter Stemmer Test Files\n",
            "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
            "  [ ] problem_reports..... Problem Report Corpus\n",
            "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
            "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
            "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
            "  [ ] pros_cons........... Pros and Cons\n",
            "Hit Enter to continue: names............... Names Corpus, Version 1.3 (1994-03-29)\n",
            "  [ ] ptb................. Penn Treebank\n",
            "  [ ] punkt............... Punkt Tokenizer Models\n",
            "  [ ] punkt_tab........... Punkt Tokenizer Models\n",
            "  [ ] qc.................. Experimental Data for Question Classification\n",
            "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
            "                           version\n",
            "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
            "                           Portuguesa)\n",
            "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
            "  [ ] sample_grammars..... Sample Grammars\n",
            "  [ ] semcor.............. SemCor 3.0\n",
            "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
            "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
            "  [ ] sentiwordnet........ SentiWordNet\n",
            "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
            "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
            "  [ ] smultron............ SMULTRON Corpus Sample\n",
            "  [ ] snowball_data....... Snowball Data\n",
            "  [ ] spanish_grammars.... Grammars for Spanish\n",
            "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
            "  [ ] stopwords........... Stopwords Corpus\n",
            "Hit Enter to continue: punkt............... Punkt Tokenizer Models\n",
            "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
            "  [ ] swadesh............. Swadesh Wordlists\n",
            "  [ ] switchboard......... Switchboard Corpus Sample\n",
            "  [ ] tagsets............. Help on Tagsets\n",
            "  [ ] tagsets_json........ Help on Tagsets (JSON)\n",
            "  [ ] timit............... TIMIT Corpus Sample\n",
            "  [ ] toolbox............. Toolbox Sample Files\n",
            "  [ ] treebank............ Penn Treebank Sample\n",
            "  [ ] twitter_samples..... Twitter Samples\n",
            "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
            "                           (Unicode Version)\n",
            "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
            "  [ ] unicode_samples..... Unicode Samples\n",
            "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
            "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
            "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
            "  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n",
            "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
            "  [ ] webtext............. Web Text Corpus\n",
            "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
            "  [ ] word2vec_sample..... Word2Vec Sample\n",
            "Hit Enter to continue: stopwords........... Stopwords Corpus\n",
            "  [ ] wordnet2021......... Open English Wordnet 2021\n",
            "  [ ] wordnet2022......... Open English Wordnet 2022\n",
            "  [ ] wordnet31........... Wordnet 3.1\n",
            "  [ ] wordnet............. WordNet\n",
            "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
            "  [ ] words............... Word Lists\n",
            "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
            "                           English Prose\n",
            "\n",
            "Collections:\n",
            "  [ ] all-corpora......... All the corpora\n",
            "  [ ] all-nltk............ All packages available on nltk_data gh-pages\n",
            "                           branch\n",
            "  [ ] all................. All packages\n",
            "  [ ] book................ Everything used in the NLTK Book\n",
            "  [ ] popular............. Popular packages\n",
            "  [ ] tests............... Packages for running tests\n",
            "  [ ] third-party......... Third-party data packages\n",
            "\n",
            "([*] marks installed packages)\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> gutenberg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "    Downloading package gutenberg to /root/nltk_data...\n",
            "      Unzipping corpora/gutenberg.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('gutenberg')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydgK4WcRZUUq",
        "outputId": "7f955d3f-5248-474f-c6b9-29c9be462925"
      },
      "id": "ydgK4WcRZUUq",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gb = nltk.corpus.gutenberg\n",
        "print(\"Gutenberg files : \", gb.fileids())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UR32HclRZVe4",
        "outputId": "a4d92c5f-7b19-45b2-d048-ec7ce8c909a0"
      },
      "id": "UR32HclRZVe4",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gutenberg files :  ['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "macbeth = nltk.corpus.gutenberg.words('shakespeare-macbeth.txt')\n"
      ],
      "metadata": {
        "id": "wagRuNDuZnDI"
      },
      "id": "wagRuNDuZnDI",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('names')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MVb3HHLZ5gi",
        "outputId": "11a37292-151e-4c9d-bd54-2ad587d3bac9"
      },
      "id": "_MVb3HHLZ5gi",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/names.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Viết chương trình Python với thư viện NLTK để liệt kê các tên của copus"
      ],
      "metadata": {
        "id": "OFlQMUAsedDN"
      },
      "id": "OFlQMUAsedDN"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "ea17e0ed",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea17e0ed",
        "outputId": "320804d5-6a9e-43b9-ca5b-1e57c4e39637"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['female.txt', 'male.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.corpus.names.fileids()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Viết chương trình Python với thư viện NLTK để liệt kê danh sách các stopword bằng các\n",
        "ngôn ngữ khác nhau.\n"
      ],
      "metadata": {
        "id": "3GyvLM9HegUz"
      },
      "id": "3GyvLM9HegUz"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "93d3cf48",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93d3cf48",
        "outputId": "a2d17097-062d-45fd-c3be-966fee487f29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['albanian',\n",
              " 'arabic',\n",
              " 'azerbaijani',\n",
              " 'basque',\n",
              " 'belarusian',\n",
              " 'bengali',\n",
              " 'catalan',\n",
              " 'chinese',\n",
              " 'danish',\n",
              " 'dutch',\n",
              " 'english',\n",
              " 'finnish',\n",
              " 'french',\n",
              " 'german',\n",
              " 'greek',\n",
              " 'hebrew',\n",
              " 'hinglish',\n",
              " 'hungarian',\n",
              " 'indonesian',\n",
              " 'italian',\n",
              " 'kazakh',\n",
              " 'nepali',\n",
              " 'norwegian',\n",
              " 'portuguese',\n",
              " 'romanian',\n",
              " 'russian',\n",
              " 'slovene',\n",
              " 'spanish',\n",
              " 'swedish',\n",
              " 'tajik',\n",
              " 'tamil',\n",
              " 'turkish']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stopwords.fileids()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Viết chương trình Python với thư viện NLTK để kiểm tra danh sách các stopword bằng các\n",
        "ngôn ngữ khác nhau."
      ],
      "metadata": {
        "id": "DuNaRkD0ehnG"
      },
      "id": "DuNaRkD0ehnG"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "4ebd0886",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ebd0886",
        "outputId": "02e5d520-2f24-4ebb-b9ab-76a69e6bfac2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'albanian': 237,\n",
              " 'arabic': 754,\n",
              " 'azerbaijani': 165,\n",
              " 'basque': 326,\n",
              " 'belarusian': 224,\n",
              " 'bengali': 398,\n",
              " 'catalan': 278,\n",
              " 'chinese': 841,\n",
              " 'danish': 94,\n",
              " 'dutch': 101,\n",
              " 'english': 198,\n",
              " 'finnish': 235,\n",
              " 'french': 157,\n",
              " 'german': 232,\n",
              " 'greek': 265,\n",
              " 'hebrew': 221,\n",
              " 'hinglish': 1036,\n",
              " 'hungarian': 199,\n",
              " 'indonesian': 758,\n",
              " 'italian': 279,\n",
              " 'kazakh': 324,\n",
              " 'nepali': 255,\n",
              " 'norwegian': 176,\n",
              " 'portuguese': 207,\n",
              " 'romanian': 356,\n",
              " 'russian': 151,\n",
              " 'slovene': 1784,\n",
              " 'spanish': 313,\n",
              " 'swedish': 114,\n",
              " 'tajik': 163,\n",
              " 'tamil': 125,\n",
              " 'turkish': 53}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "stopwords_count = {lang: len(stopwords.words(lang)) for lang in stopwords.fileids()}\n",
        "\n",
        "stopwords_count\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Viết chương trình Python với thư viện NLTK để loại bỏ các stopword từ một văn bản đã cho."
      ],
      "metadata": {
        "id": "U-VOq8w0eklL"
      },
      "id": "U-VOq8w0eklL"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "8694255e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8694255e",
        "outputId": "a7193822-fb02-4ef6-f677-c9b8695165a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Don’t', 'wait', 'opportunity,', 'create']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "text = \"Don’t wait for an opportunity, create it\"\n",
        "words = text.split()\n",
        "filtered_words = [word for word in words if word.lower() not in stopwords.words('english')]\n",
        "\n",
        "filtered_words\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Viết chương trình Python với thư viện NLTK bỏ qua các stopword từ danh sách các\n",
        "stopword."
      ],
      "metadata": {
        "id": "Y23gVci3elVJ"
      },
      "id": "Y23gVci3elVJ"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "68775bb3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68775bb3",
        "outputId": "7de249f1-42f2-49c7-ee79-856a5417d97a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " \"he'd\",\n",
              " \"he'll\",\n",
              " \"he's\",\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " \"i'd\",\n",
              " \"i'll\",\n",
              " \"i'm\",\n",
              " \"i've\",\n",
              " 'if',\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it'd\",\n",
              " \"it'll\",\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'nor',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she'd\",\n",
              " \"she'll\",\n",
              " \"she's\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " \"they'd\",\n",
              " \"they'll\",\n",
              " \"they're\",\n",
              " \"they've\",\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " \"we'd\",\n",
              " \"we'll\",\n",
              " \"we're\",\n",
              " \"we've\",\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves'}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "custom_stopwords = set(stopwords.words('english')) - {\"not\", \"no\"}\n",
        "\n",
        "custom_stopwords\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Viết một chương trình Python với thư viện NLTK để tìm định nghĩa và ví dụ của một từ đã\n",
        "cho bằng WordNet từ Wikipedia,"
      ],
      "metadata": {
        "id": "7fu0NWj_enkC"
      },
      "id": "7fu0NWj_enkC"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "42af6754",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42af6754",
        "outputId": "05b68f98-7858-4ccd-b6aa-2761712f5520"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('a machine for performing calculations automatically', []),\n",
              " ('an expert at calculation (or at operating calculating machines)', [])]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "from nltk.corpus import wordnet\n",
        "nltk.download('wordnet')\n",
        "\n",
        "word = \"computer\"\n",
        "synsets = wordnet.synsets(word)\n",
        "\n",
        "[(syn.definition(), syn.examples()) for syn in synsets]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Viết chương trình Python với thư viện NLTK để tìm tập hợp các từ đồng nghĩa và trái nghĩa\n",
        "của một từ nào đó."
      ],
      "metadata": {
        "id": "iUMB4t1Bepqx"
      },
      "id": "iUMB4t1Bepqx"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "0ca95fd9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ca95fd9",
        "outputId": "c3e935b7-4cb2-4be1-9101-207f2f821133"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'felicitous', 'glad', 'happy', 'well-chosen'}, {'unhappy'})"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "synonyms = set()\n",
        "antonyms = set()\n",
        "\n",
        "for syn in wordnet.synsets(\"happy\"):\n",
        "    for lemma in syn.lemmas():\n",
        "        synonyms.add(lemma.name())\n",
        "        if lemma.antonyms():\n",
        "            antonyms.add(lemma.antonyms()[0].name())\n",
        "\n",
        "synonyms, antonyms\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Viết chương trình Python với thư viện NLTK để có cái nhìn tổng quan về bộ tag, chi tiết của\n",
        "một tag cụ thể trong bộ tag và chi tiết về một số bộ tag liên quan, sử dụng biểu thức chính\n",
        "quy.\n"
      ],
      "metadata": {
        "id": "YObggb_vesLb"
      },
      "id": "YObggb_vesLb"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "803f5f16",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "803f5f16",
        "outputId": "8040812a-acab-4ade-932d-baceab76d0ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Unzipping help/tagsets.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['PRP$', 'VBG', 'FW', 'VB', 'POS', \"''\", 'VBP', 'VBN', 'JJ', 'WP', 'VBZ', 'DT', 'RP', '$', 'NN', ')', '(', 'RBR', 'VBD', ',', '.', 'TO', 'LS', 'RB', ':', 'NNS', 'NNP', '``', 'WRB', 'CC', 'PDT', 'RBS', 'PRP', 'CD', 'EX', 'IN', 'WP$', 'MD', 'NNPS', '--', 'JJS', 'JJR', 'SYM', 'UH', 'WDT'])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "nltk.download('tagsets')\n",
        "\n",
        "nltk.data.load('help/tagsets/upenn_tagset.pickle').keys()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Viết chương trình Python với thư viện NLTK để so sánh sự giống nhau của hai danh từ đã\n",
        "cho."
      ],
      "metadata": {
        "id": "6-K9lEoeeucp"
      },
      "id": "6-K9lEoeeucp"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "663ecdbf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "663ecdbf",
        "outputId": "c88ff258-60c1-4d8d-bb7c-68d043b18d12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet_ic.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7.591401417609093"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "nltk.download('wordnet_ic')\n",
        "from nltk.corpus import wordnet_ic\n",
        "\n",
        "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
        "\n",
        "word1 = wordnet.synset('car.n.01')\n",
        "word2 = wordnet.synset('automobile.n.01')\n",
        "\n",
        "word1.res_similarity(word2, brown_ic)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Viết chương trình Python với thư viện NLTK để so sánh sự giống nhau của hai động từ đã\n",
        "cho."
      ],
      "metadata": {
        "id": "Sfx5NIowewEP"
      },
      "id": "Sfx5NIowewEP"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "9ef28503",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ef28503",
        "outputId": "aa71ba22-ec8c-4a3b-9d9f-091372722622"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8571428571428571"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "word1 = wordnet.synset('run.v.01')\n",
        "word2 = wordnet.synset('sprint.v.01')\n",
        "\n",
        "word1.wup_similarity(word2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Viết chương trình Python với thư viện NLTK để tìm số lượng tên nam và nữ trong các tên\n",
        "kho ngữ liệu. In tên 10 nam và nữ đầu tiên. Lưu ý: Kho văn bản tên chứa tổng cộng khoảng\n",
        "2943 nam (male.txt) và 5001 nữ (Female.txt) tên. Kho được biên soạn bởi Kantrowitz, Ross."
      ],
      "metadata": {
        "id": "0NAEA_rrexZ_"
      },
      "id": "0NAEA_rrexZ_"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "582acb2f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "582acb2f",
        "outputId": "6bede3c9-015d-4eb6-997b-83c07ce2bcb0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2943,\n",
              " 5001,\n",
              " ['Aamir',\n",
              "  'Aaron',\n",
              "  'Abbey',\n",
              "  'Abbie',\n",
              "  'Abbot',\n",
              "  'Abbott',\n",
              "  'Abby',\n",
              "  'Abdel',\n",
              "  'Abdul',\n",
              "  'Abdulkarim'],\n",
              " ['Abagael',\n",
              "  'Abagail',\n",
              "  'Abbe',\n",
              "  'Abbey',\n",
              "  'Abbi',\n",
              "  'Abbie',\n",
              "  'Abby',\n",
              "  'Abigael',\n",
              "  'Abigail',\n",
              "  'Abigale'])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "male_names = nltk.corpus.names.words('male.txt')\n",
        "female_names = nltk.corpus.names.words('female.txt')\n",
        "\n",
        "(len(male_names), len(female_names), male_names[:10], female_names[:10])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Viết chương trình Python với thư viện NLTK để in 15 kết hợp ngẫu nhiên đầu tiên được gắn\n",
        "nhãn nam và được gắn nhãn tên nữ từ kho tên."
      ],
      "metadata": {
        "id": "nViKM6wze0Hm"
      },
      "id": "nViKM6wze0Hm"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "1befe157",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1befe157",
        "outputId": "dce23beb-cf76-4938-e7d1-7f5c4357e706"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Bertie',\n",
              " 'Ambrosi',\n",
              " 'Creighton',\n",
              " 'Darrell',\n",
              " 'Town',\n",
              " 'Alejandro',\n",
              " 'Alfonse',\n",
              " 'Kayla',\n",
              " 'Gisela',\n",
              " 'Else',\n",
              " 'Elspeth',\n",
              " 'Ailey',\n",
              " 'Jobi',\n",
              " 'Caria',\n",
              " 'Cam']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "random.sample(male_names, 7) + random.sample(female_names, 8)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. Viết chương trình Python với thư viện NLTK để trích xuất ký tự cuối cùng của tất cả các tên\n",
        "được gắn nhãn và tạo mảng mới với chữ cái cuối cùng của mỗi tên và nhãn được liên kết."
      ],
      "metadata": {
        "id": "BQGmDDsxe2t-"
      },
      "id": "BQGmDDsxe2t-"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "cf5a561d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf5a561d",
        "outputId": "1d8352fc-f3d6-46d2-a71e-0d70de5fc435"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('r', 'male'),\n",
              " ('n', 'male'),\n",
              " ('y', 'male'),\n",
              " ('e', 'male'),\n",
              " ('t', 'male'),\n",
              " ('t', 'male'),\n",
              " ('y', 'male'),\n",
              " ('l', 'male'),\n",
              " ('l', 'male'),\n",
              " ('m', 'male'),\n",
              " ('l', 'female'),\n",
              " ('l', 'female'),\n",
              " ('e', 'female'),\n",
              " ('y', 'female'),\n",
              " ('i', 'female'),\n",
              " ('e', 'female'),\n",
              " ('y', 'female'),\n",
              " ('l', 'female'),\n",
              " ('l', 'female'),\n",
              " ('e', 'female')]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "[(name[-1], 'male') for name in male_names[:10]] + [(name[-1], 'female') for name in female_names[:10]]\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}